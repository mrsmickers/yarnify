import { VoiceEventData, Voice, VoiceEventType, ReadableStreamType, VoiceMetadata } from '@voltagent/core';
export { ReadableStreamType, Voice, VoiceEventData, VoiceEventType, VoiceMetadata, VoiceOptions } from '@voltagent/core';
import { SpeechCreateParams } from 'openai/resources/audio/speech';
import { TranscriptionCreateParams } from 'openai/resources/audio/transcriptions';

type BaseVoiceProviderOptions = {
    apiKey?: string;
    options?: Record<string, unknown>;
};
type BaseVoiceProviderEvents = VoiceEventData;

declare abstract class BaseVoiceProvider implements Voice {
    protected options: BaseVoiceProviderOptions;
    protected eventListeners: Map<VoiceEventType, Set<Function>>;
    constructor(options?: BaseVoiceProviderOptions);
    abstract speak(text: string | NodeJS.ReadableStream, options?: {
        voice?: string;
        speed?: number;
        pitch?: number;
    }): Promise<NodeJS.ReadableStream>;
    abstract listen(audio: NodeJS.ReadableStream, options?: {
        language?: string;
        model?: string;
        stream?: boolean;
    }): Promise<string | ReadableStreamType>;
    abstract connect(options?: Record<string, unknown>): Promise<void>;
    abstract disconnect(): void;
    abstract send(audioData: NodeJS.ReadableStream | Int16Array): Promise<void>;
    abstract getVoices(): Promise<Array<{
        id: string;
        name: string;
        language: string;
        gender?: "male" | "female" | "neutral";
        metadata?: Record<string, unknown>;
    }>>;
    protected emit<E extends VoiceEventType>(event: E, data: BaseVoiceProviderEvents[E]): void;
    on<E extends VoiceEventType>(event: E, callback: (data: BaseVoiceProviderEvents[E]) => void): void;
    off<E extends VoiceEventType>(event: E, callback: (data: BaseVoiceProviderEvents[E]) => void): void;
}

declare const OPENAI_VOICES: readonly ["alloy", "echo", "fable", "onyx", "nova", "shimmer", "ash", "coral", "sage"];
type OpenAIVoice = (typeof OPENAI_VOICES)[number];
/**
 * OpenAI voice options
 */
type OpenAIVoiceOptions = BaseVoiceProviderOptions & {
    /**
     * OpenAI API key
     */
    apiKey: string;
    /**
     * Model to use for speech recognition
     * @default "whisper-1"
     */
    speechModel?: string;
    /**
     * Model to use for text-to-speech
     * @default "tts-1"
     */
    ttsModel?: string;
    /**
     * Voice to use for text-to-speech
     * @default "alloy"
     */
    voice?: OpenAIVoice;
    /**
     * Additional OpenAI API options
     */
    options?: {
        /**
         * Organization ID
         */
        organization?: string;
        /**
         * Request timeout in milliseconds
         */
        timeout?: number;
        /**
         * Maximum retries for failed requests
         */
        maxRetries?: number;
    };
};
/**
 * Options for text-to-speech
 */
type OpenAISpeakOptions = Omit<SpeechCreateParams, "model" | "voice" | "input">;
/**
 * Options for speech-to-text
 */
type OpenAIListenOptions = Omit<TranscriptionCreateParams, "model" | "file" | "stream"> & {
    /**
     * Whether to stream the transcription
     * @default false
     */
    stream?: boolean;
};
/**
 * Supported audio formats
 */
type OpenAIAudioFormat = "mp3" | "mp4" | "mpeg" | "mpga" | "m4a" | "wav" | "webm";

declare class OpenAIVoiceProvider extends BaseVoiceProvider {
    private readonly client;
    private readonly speechModel;
    private readonly ttsModel;
    private readonly voice;
    constructor(options: OpenAIVoiceOptions);
    /**
     * Convert text to speech
     * @param text Text to convert to speech
     * @param options Options for text-to-speech
     * @returns Audio stream
     */
    speak(text: string | NodeJS.ReadableStream, options?: OpenAISpeakOptions & {
        voice?: OpenAIVoiceOptions["voice"];
    }): Promise<NodeJS.ReadableStream>;
    /**
     * Convert speech to text
     * @param audio Audio stream to transcribe
     * @param options Options for speech-to-text
     * @returns Transcribed text or stream of transcribed text
     */
    listen(audio: NodeJS.ReadableStream, options?: OpenAIListenOptions & {
        format?: OpenAIAudioFormat;
        stream?: boolean;
    }): Promise<string | ReadableStreamType>;
    connect(): Promise<void>;
    disconnect(): void;
    send(): Promise<void>;
    getVoices(): Promise<VoiceMetadata[]>;
}

declare const ELEVENLABS_MODELS: readonly ["eleven_multilingual_v2", "eleven_flash_v2_5", "eleven_flash_v2", "eleven_multilingual_sts_v2", "eleven_english_sts_v2", "scribe_v1"];
type ElevenLabsModel = (typeof ELEVENLABS_MODELS)[number];
/**
 * ElevenLabs voice options
 */
type ElevenLabsVoiceOptions = BaseVoiceProviderOptions & {
    /**
     * ElevenLabs API key
     */
    apiKey: string;
    /**
     * Model to use for speech recognition
     * @default "scribe_v1"
     */
    speechModel?: ElevenLabsModel;
    /**
     * Model to use for text-to-speech
     * @default "eleven_multilingual_v2"
     */
    ttsModel?: ElevenLabsModel;
    /**
     * Voice ID to use for text-to-speech
     * @default "Callum"
     */
    voice?: string;
    /**
     * Additional ElevenLabs API options
     */
    options?: {
        /**
         * Request timeout in milliseconds
         */
        timeout?: number;
        /**
         * Maximum retries for failed requests
         */
        maxRetries?: number;
        /**
         * Voice stability (0-1)
         * @default 0.5
         */
        stability?: number;
        /**
         * Voice similarity boost (0-1)
         * @default 0.75
         */
        similarityBoost?: number;
        /**
         * Voice style (0-1)
         * @default 0
         */
        style?: number;
        /**
         * Whether to use speaker boost
         * @default true
         */
        useSpeakerBoost?: boolean;
    };
};

declare class ElevenLabsVoiceProvider extends BaseVoiceProvider {
    private readonly client;
    private readonly speechModel;
    private readonly ttsModel;
    private readonly voice;
    private readonly voiceSettings;
    constructor(options: ElevenLabsVoiceOptions);
    speak(input: string | NodeJS.ReadableStream, options?: {
        voice?: string;
        stability?: number;
        similarityBoost?: number;
        style?: number;
        useSpeakerBoost?: boolean;
    }): Promise<NodeJS.ReadableStream>;
    listen(audio: NodeJS.ReadableStream, options?: {
        language?: string;
        model?: ElevenLabsModel;
        tagAudioEvents?: boolean;
        numSpeakers?: number;
        fileType?: string;
    }): Promise<string>;
    connect(): Promise<void>;
    disconnect(): void;
    send(): Promise<void>;
    getVoices(): Promise<VoiceMetadata[]>;
}

type XsaiVoiceOptions = BaseVoiceProviderOptions & {
    /** xsAI dashboard key */
    apiKey: string;
    /** xsAI base URL – defaults to `"https://api.openai.com/v1"` */
    baseURL?: string;
    /** Model *id* for TTS (required by xsAI) – default `"tts-1"` */
    ttsModel?: string;
    /** Model *id* for STT (required by xsAI) – default `"whisper-1"` */
    speechModel?: string;
    /** Voice ID (library‑specific) – defaults to `"alloy"` */
    voice?: string;
    /** Extra per‑provider knobs */
    options?: {
        headers?: Record<string, string>;
    };
};
type XsaiSpeakOptions = {
    voice?: string;
    /** @default `"mp3"` */
    format?: "aac" | "flac" | "mp3" | "opus" | "pcm" | "wav";
    /** @default `1.0` */
    speed?: number;
};
type XsaiListenOptions = {
    language?: string;
    prompt?: string;
    temperature?: string;
    /** custom filename hint for the Blob sent to xsAI */
    fileName?: string;
};

declare class XsAIVoiceProvider extends BaseVoiceProvider {
    private readonly apiKey;
    private readonly baseURL;
    private readonly ttsModel;
    private readonly speechModel;
    private readonly voice;
    private readonly headers?;
    constructor(options: XsaiVoiceOptions);
    speak(input: string | NodeJS.ReadableStream, opts?: XsaiSpeakOptions): Promise<NodeJS.ReadableStream>;
    listen(audio: NodeJS.ReadableStream, opts?: XsaiListenOptions): Promise<string | ReadableStreamType>;
    connect(): Promise<void>;
    disconnect(): void;
    send(): Promise<void>;
    getVoices(): Promise<VoiceMetadata[]>;
}

export { BaseVoiceProvider, ElevenLabsVoiceProvider, OpenAIVoiceProvider, XsAIVoiceProvider };
